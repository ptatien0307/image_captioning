{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":7689108,"sourceType":"datasetVersion","datasetId":4487129}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"transformer","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/transformer-ab249c4e-b94d-4d51-8be3-83d33c508ee5.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240225/auto/storage/goog4_request&X-Goog-Date=20240225T094814Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=7cbf97e1140a28cfbfca4d1c31ddcb9973ce27eb8e3eb99fe2c573ca560945a038fa32efd31da5c56b0f7cf645ef36a390ece17b589aab3619892ce6ee4bdffb05fd1302ae425b7cd18364feaa2390da3f4c8f140b62edee457861c9e1e7d61faf5ad841f5148a1434d2746122d5f952205c05afd2d1d0ee794402e41150c891ad6eb28ef85f8dd4e8755f6f3c0e68a47ee967a6cce1b90697beaf032fb373d51fb2bb866259d4f95f5854aa8b65bd490870821ab4ad73aae96b234992b9550ede9a0607691667801248cbd1135a12dcfe366c61df69c6feb099bd947f496ba69a19911b6dd8debf1c05085ff423cf65c76228755ada9f7e6bc2b8baf3697bb5","timestamp":1708854516640}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import re\n","import cv2\n","import math\n","import glob\n","import spacy\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import tensorflow as tf\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","\n","import torch\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torch.nn import TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","!python -m spacy download en_core_web_sm\n","clear_output()"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:30:51.436163Z","iopub.execute_input":"2024-02-25T07:30:51.43689Z","iopub.status.idle":"2024-02-25T07:31:16.591727Z","shell.execute_reply.started":"2024-02-25T07:30:51.436855Z","shell.execute_reply":"2024-02-25T07:31:16.590574Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"pnDwWpEtqkXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"1ZEwbIwTqkXk"}},{"cell_type":"code","source":["spacy_eng = spacy.load(\"en_core_web_sm\")\n","class Vocabulary:\n","    def __init__(self,freq_threshold):\n","        # Setting the pre-reserved tokens int to string tokens\n","        self.index2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n","\n","        # String to int tokens\n","        # Tts reverse dict self.index2word\n","        self.word2index = {v: k for k, v in self.index2word.items()}\n","\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.index2word)\n","\n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","\n","    def build_vocab(self, sentence_list):\n","        frequencies = Counter()\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","\n","                #add the word to the vocab if it reaches minum frequecy threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.word2index[word] = idx\n","                    self.index2word[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self,text):\n","        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [self.word2index[token] if token in self.word2index else self.word2index[\"<UNK>\"] for token in tokenized_text ]"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:16.59356Z","iopub.execute_input":"2024-02-25T07:31:16.59459Z","iopub.status.idle":"2024-02-25T07:31:18.108885Z","shell.execute_reply.started":"2024-02-25T07:31:16.59456Z","shell.execute_reply":"2024-02-25T07:31:18.108106Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"JRJ2g9hOqkXm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageCaptioningDataset(Dataset):\n","    \"\"\"Image Captioning dataset\"\"\"\n","\n","    def __init__(self, csv_file, transform, max_length, freq_threshold=5):\n","        self.dataframe = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.max_length = max_length\n","\n","        self.images = self.dataframe['image']\n","        self.captions = self.dataframe['caption']\n","\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.captions.tolist())\n","\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        caption = self.captions[idx]\n","        image_path = self.images[idx]\n","\n","        image = cv2.imread(f'/kaggle/input/flickr8k/Images/{image_path}')\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Tokenize caption\n","        caption_tokens = []\n","        caption_tokens += [self.vocab.word2index[\"<SOS>\"]]\n","        caption_tokens += self.vocab.numericalize(caption)\n","        caption_tokens += [self.vocab.word2index[\"<EOS>\"]]\n","\n","        input_tokens = caption_tokens[:-1].copy() # input\n","        target_tokens = caption_tokens[1:].copy() # target\n","\n","        # Padding input tokens\n","        cap_length = len(input_tokens)\n","        padding_size = self.max_length - cap_length\n","        input_tokens += [0] * padding_size\n","        target_tokens += [0] * padding_size\n","\n","\n","        # Create padding mask\n","        padding_mask = torch.ones([self.max_length, ])\n","        padding_mask[:cap_length] = 0.0\n","        padding_mask = padding_mask.bool()\n","\n","        input_tokens = torch.tensor(input_tokens) # input\n","        target_tokens = torch.tensor(target_tokens) # target\n","\n","        return image, input_tokens, target_tokens, padding_mask"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.109966Z","iopub.execute_input":"2024-02-25T07:31:18.110241Z","iopub.status.idle":"2024-02-25T07:31:18.121039Z","shell.execute_reply.started":"2024-02-25T07:31:18.110216Z","shell.execute_reply":"2024-02-25T07:31:18.120052Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"amBhAKfdqkXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"SYNB8Z9OqkXo"}},{"cell_type":"code","source":["class Encoder(torch.nn.Module):\n","    def __init__(self, encoder_dim, d_model):\n","        super(Encoder, self).__init__()\n","\n","        # Load pretrained model and remove last fc layer\n","        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-2]).to(device)\n","\n","        # Freeze layer\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        self.linear = torch.nn.Linear(encoder_dim, d_model).to(device)\n","        self.dropout = torch.nn.Dropout(0.2)\n","    def forward(self, images):\n","        images = images.to(device)\n","\n","        features = self.model(images)\n","        features = features.view(features.size(0), features.size(1), -1)\n","        features = features.permute(0, 2, 1)\n","        features = self.dropout(features)\n","        features = self.linear(features)\n","        return features # (batch_size, 49, d_model)"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.123706Z","iopub.execute_input":"2024-02-25T07:31:18.124477Z","iopub.status.idle":"2024-02-25T07:31:18.135717Z","shell.execute_reply.started":"2024-02-25T07:31:18.124451Z","shell.execute_reply":"2024-02-25T07:31:18.135011Z"},"trusted":true,"id":"g2Rj5I7xqkXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=50):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = torch.nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        self.pe = torch.zeros(1, max_len, d_model).to(device)\n","        self.pe[0, :, 0::2] = torch.sin(position * div_term)\n","        self.pe[0, :, 1::2] = torch.cos(position * div_term)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Arguments:\n","            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n","        \"\"\"\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.136794Z","iopub.execute_input":"2024-02-25T07:31:18.137126Z","iopub.status.idle":"2024-02-25T07:31:18.149395Z","shell.execute_reply.started":"2024-02-25T07:31:18.137095Z","shell.execute_reply":"2024-02-25T07:31:18.148484Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"ayuSmXXHqkXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(torch.nn.Module):\n","\n","    def __init__(self, n_tokens, d_model,\n","                 n_heads, dim_forward,\n","                 n_layers, dropout = 0.2):\n","        super(Decoder, self).__init__()\n","        self.embedding = torch.nn.Embedding(n_tokens, d_model) # embedding layer\n","        self.pos_encoder = PositionalEncoding(d_model, dropout) # positional encoder\n","\n","        decoder_layers = TransformerDecoderLayer(d_model, n_heads, dim_forward, dropout, batch_first=True) # encoder layer\n","        self.transformer_decoder = TransformerDecoder(decoder_layers, n_layers) # transformer encoder\n","\n","\n","        self.d_model = d_model # number of features\n","        self.linear = torch.nn.Linear(d_model, n_tokens) # last linear model for prediction\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","    def forward(self, features, captions, padding_mask, captions_mask=None):\n","        \"\"\"\n","        Arguments:\n","            captions: Tensor, shape ``[batch_size, seq_len]``\n","            captions_mask: Tensor, shape ``[seq_len, seq_len]``\n","\n","        Returns:\n","            output Tensor of shape ``[batch_size, seq_len, n_tokens]``\n","        \"\"\"\n","        captions = captions\n","        captions = self.embedding(captions)\n","        captions = captions * math.sqrt(self.d_model)\n","        captions = self.pos_encoder(captions)\n","\n","        if captions_mask is None:\n","            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n","            Unmasked positions are filled with float(0.0).\n","            \"\"\"\n","            captions_mask = torch.nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(device)\n","\n","        output = self.transformer_decoder(tgt=captions,\n","                                          memory=features,\n","                                          tgt_key_padding_mask=padding_mask,\n","                                          tgt_mask=captions_mask)\n","        output = self.dropout(output)\n","        output = self.linear(output)\n","        return output\n","\n","\n","    def predict(self, feature, max_length, vocab):\n","        word = torch.tensor([vocab.word2index['<SOS>']] + [0] * (max_length - 1)).view(1, -1).to(device)\n","        padding_mask = torch.Tensor([True] * max_length).view(1, -1).to(device)\n","\n","        predicted_captions = []\n","\n","        for i in range(max_length - 1):\n","            # Update the padding masks\n","            padding_mask[:, i] = False\n","\n","            # Get the model prediction for the next word\n","            output = self.forward(feature, word, padding_mask)\n","            output = output[0, i]\n","            predicted_word_idx = output.argmax(dim=-1)\n","            predicted_captions.append(predicted_word_idx.item())\n","            word[:, i + 1] = predicted_word_idx.item()\n","\n","            # End if <EOS> appears\n","            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n","                break\n","\n","        return ' '.join([vocab.index2word[idx] for idx in predicted_captions])"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.150497Z","iopub.execute_input":"2024-02-25T07:31:18.150723Z","iopub.status.idle":"2024-02-25T07:31:18.168818Z","shell.execute_reply.started":"2024-02-25T07:31:18.150703Z","shell.execute_reply":"2024-02-25T07:31:18.167897Z"},"trusted":true,"id":"JhdtWeV6qkXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Captioner(torch.nn.Module):\n","    def __init__(self, n_tokens, d_model, n_heads, dim_forward, n_layers, encoder_dim, vocab):\n","        super(Captioner, self).__init__()\n","        self.encoder =  Encoder(encoder_dim=encoder_dim,\n","                                d_model=d_model)\n","        self.decoder = Decoder(n_tokens=n_tokens,\n","                               d_model=d_model,\n","                               n_heads=n_heads,\n","                               dim_forward=dim_forward,\n","                               n_layers=n_layers)\n","\n","        self.vocab = vocab\n","\n","    def forward(self, images, captions, padding_mask):\n","\n","        features = self.encoder(images)\n","        output = self.decoder(features, captions, padding_mask)\n","\n","        return output\n","\n","    def generate_caption(self, image, max_length=50):\n","        image = image.to(device)\n","        features = self.encoder(image)\n","        predicted_caption = self.decoder.predict(features, max_length, self.vocab)\n","        return predicted_caption"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.169952Z","iopub.execute_input":"2024-02-25T07:31:18.170204Z","iopub.status.idle":"2024-02-25T07:31:18.182168Z","shell.execute_reply.started":"2024-02-25T07:31:18.170183Z","shell.execute_reply":"2024-02-25T07:31:18.181315Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"DyX6gtMRqkXp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"JYzM5nUxqkXp"}},{"cell_type":"code","source":["def get_dataset_loader(file, transform, batch_size, max_length, freq_threshold):\n","    dataset = ImageCaptioningDataset(\n","                        csv_file=file,\n","                        transform=transform,\n","                        max_length=max_length,\n","                        freq_threshold=freq_threshold)\n","\n","    dataloader = DataLoader(\n","                    dataset=dataset,\n","                    batch_size=batch_size,\n","                    shuffle=True,\n","                    num_workers=2)\n","\n","    return dataset, dataloader"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.183131Z","iopub.execute_input":"2024-02-25T07:31:18.183393Z","iopub.status.idle":"2024-02-25T07:31:18.191939Z","shell.execute_reply.started":"2024-02-25T07:31:18.183369Z","shell.execute_reply":"2024-02-25T07:31:18.191116Z"},"trusted":true,"id":"Ij9_lY9bqkXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer():\n","    def __init__(self, config, train_dataset, train_loader, test_loader,\n","                 model, optimizer, criterion,\n","                 save_every, output_dir):\n","        self.config = config\n","        self.train_dataset = train_dataset\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","\n","        self.vocab_size = len(self.train_dataset.vocab)\n","        self.vocab = self.train_dataset.vocab\n","\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","\n","        self.save_every = save_every\n","        self.output_dir = output_dir\n","\n","    def run_epoch(self, epoch):\n","        epoch_loss = []\n","        self.model.train()\n","        pbar = tqdm(enumerate(iter(self.train_loader)), position=0, leave=True)\n","        for idx, (images, captions, targets, padding_mask) in pbar:\n","            images, captions, targets, padding_mask = images.to(device), captions.to(device), targets.to(device), padding_mask.to(device)\n","\n","            # Forward\n","            outputs = self.model(images, captions, padding_mask)\n","            loss = self.criterion(outputs.view(-1, len(self.train_dataset.vocab)), targets.reshape(-1))\n","            epoch_loss.append(loss.item())\n","\n","            # Backward and update params\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            # Progess bar\n","            pbar.set_postfix_str(f\"{epoch}/{self.config.num_epochs} - Train loss: {sum(epoch_loss) / len(epoch_loss):0.4f}\")\n","        return sum(epoch_loss) / len(epoch_loss)\n","\n","\n","    def test(self, epoch):\n","        with torch.no_grad():\n","            epoch_loss = []\n","            self.model.eval()\n","            pbar = tqdm(enumerate(iter(self.test_loader)), position=0, leave=True)\n","            for idx, (images, captions, targets, padding_mask) in pbar:\n","                images, captions, targets, padding_mask = images.to(device), captions.to(device), targets.to(device), padding_mask.to(device)\n","\n","                # Forward\n","                outputs = self.model(images, captions, padding_mask)\n","                loss = self.criterion(outputs.view(-1, len(self.train_dataset.vocab)), targets.reshape(-1))\n","                epoch_loss.append(loss.item())\n","\n","                # Progess bar\n","                pbar.set_postfix_str(f\"{epoch}/{self.config.num_epochs} - Test loss: {sum(epoch_loss) / len(epoch_loss):0.4f}\")\n","            return sum(epoch_loss) / len(epoch_loss)\n","\n","    def train(self, resume=False):\n","        start_epoch = 0  # Starting epoch\n","        if resume:\n","            # Load model and optimizer state\n","            model_state, optimizer_state, prev_epoch, prev_loss = self.load_model()\n","            model.load_state_dict(model_state)\n","            optimizer.load_state_dict(optimizer_state)\n","\n","            # Starting epoch\n","            start_epoch = prev_epoch\n","        for epoch in range(start_epoch + 1, self.config.num_epochs + 1):\n","            train_loss = self.run_epoch(epoch)\n","            test_loss = self.test(epoch)\n","\n","            # Save model\n","            if epoch % self.save_every == 0:\n","                self.save_model(model, optimizer, epoch, test_loss)\n","\n","    def save_model(self, model, optimizer, epoch, loss):\n","        model_state = {\n","            'epoch': epoch,\n","            'loss': loss,\n","            'd_model': self.config.d_model,\n","            'n_tokens': len(self.train_dataset.vocab),\n","            'n_heads': self.config.n_heads,\n","            'dim_forward': self.config.dim_forward,\n","            'n_layers': self.config.n_layers,\n","            'encoder_dim': self.config.encoder_dim,\n","            'vocab': self.train_dataset.vocab,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict()\n","        }\n","\n","        torch.save(model_state, f'{self.output_dir}/trans2_{epoch}_{loss}.pth')\n","\n","\n","    def load_model(self):\n","        checkpoint = torch.load()\n","\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","\n","        model_state = checkpoint['model_state_dict']\n","        optimizer_state = checkpoint['optimizer_state_dict']\n","\n","        return model_state, optimizer_state, epoch, loss"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.193141Z","iopub.execute_input":"2024-02-25T07:31:18.193416Z","iopub.status.idle":"2024-02-25T07:31:18.213111Z","shell.execute_reply.started":"2024-02-25T07:31:18.193393Z","shell.execute_reply":"2024-02-25T07:31:18.212281Z"},"trusted":true,"id":"CnEufI2ZqkXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class config:\n","    train_file = \"/kaggle/input/flickr8k-split/train.csv\"\n","    test_file = \"/kaggle/input/flickr8k-split/test.csv\"\n","    output_dir = '/kaggle/working'\n","    d_model = 400\n","    n_heads = 4\n","    dim_forward = 512\n","    n_layers = 2\n","    encoder_dim = 2048\n","\n","    num_epochs = 100\n","    batch_size = 64\n","    lr = 5e-6\n","    max_length = 50\n","    save_every = 5\n","    freq_threshold = 5\n","\n","    transform = transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Resize(232, antialias=True),\n","                        transforms.CenterCrop(224),\n","                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                            std=[0.229, 0.224, 0.225])])"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:18.21556Z","iopub.execute_input":"2024-02-25T07:31:18.215821Z","iopub.status.idle":"2024-02-25T07:31:18.226959Z","shell.execute_reply.started":"2024-02-25T07:31:18.215799Z","shell.execute_reply":"2024-02-25T07:31:18.226054Z"},"trusted":true,"id":"PKs8Jq-yqkXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset, train_loader = get_dataset_loader(config.train_file,\n","                                     config.transform,\n","                                     config.batch_size,\n","                                     config.max_length,\n","                                     config.freq_threshold)\n","\n","\n","test_dataset, test_loader = get_dataset_loader(config.test_file,\n","                                     config.transform,\n","                                     config.batch_size,\n","                                     config.max_length,\n","                                     config.freq_threshold)\n","\n","\n","model = Captioner(\n","        n_tokens=len(train_dataset.vocab),\n","        d_model=config.d_model,\n","        n_heads=config.n_heads,\n","        dim_forward=config.dim_forward,\n","        n_layers=config.n_layers,\n","        encoder_dim=config.encoder_dim,\n","        vocab=train_dataset.vocab\n","    )\n","\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=config.lr,\n","                             weight_decay=0.00001)\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n","\n","\n","trainer = Trainer(config, train_dataset, train_loader, test_loader,\n","                  model, optimizer, criterion,\n","                  config.save_every, config.output_dir)\n","trainer.train(resume=False)"],"metadata":{"trusted":true,"id":"3GAjiPJkqkXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model(path):\n","    checkpoint = torch.load(path)\n","    model = Captioner(\n","        n_tokens=checkpoint['n_tokens'],\n","        d_model=checkpoint['d_model'],\n","        n_heads=checkpoint['n_heads'],\n","        dim_forward=checkpoint['dim_forward'],\n","        n_layers=checkpoint['n_layers'],\n","        encoder_dim=checkpoint['encoder_dim'],\n","        vocab=checkpoint['vocab'],\n","\n","    )\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    return model\n","\n","def plot_result(image, caption):\n","    plt.imshow(image)\n","    plt.axis(\"off\")\n","    plt.title(caption)"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:22.038692Z","iopub.execute_input":"2024-02-25T07:31:22.039665Z","iopub.status.idle":"2024-02-25T07:31:22.047553Z","shell.execute_reply.started":"2024-02-25T07:31:22.039624Z","shell.execute_reply":"2024-02-25T07:31:22.04647Z"},"trusted":true,"id":"Cg8Voc8oqkXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model(\"models/transformer2/model_best.pth\")\n","model.eval()\n","transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize(232, antialias=True),\n","        transforms.CenterCrop(224),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                            std=[0.229, 0.224, 0.225])\n","        ])\n","images = glob.glob(\"/kaggle/input/flickr8k/Images/*.jpg\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:31:22.232735Z","iopub.execute_input":"2024-02-25T07:31:22.23312Z","iopub.status.idle":"2024-02-25T07:31:23.018046Z","shell.execute_reply.started":"2024-02-25T07:31:22.233091Z","shell.execute_reply":"2024-02-25T07:31:23.016791Z"},"trusted":true,"id":"FIaEEVkkqkXr","outputId":"33970043-880c-4ded-ce94-400400d0bb1e"},"execution_count":null,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/transformer2/model_best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      5\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m232\u001b[39m, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                             std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m      9\u001b[0m         ])\n","Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[0;32m----> 2\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m Captioner(\n\u001b[1;32m      4\u001b[0m         n_tokens\u001b[38;5;241m=\u001b[39mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m         d_model\u001b[38;5;241m=\u001b[39mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/transformer2/model_best.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'models/transformer2/model_best.pth'","output_type":"error"}]},{"cell_type":"code","source":["with torch.no_grad():\n","    idx = random.randint(0, len(images))\n","    image_path = images[idx]\n","    name_path = image_path.split('/')[-1]\n","    labels = df[df['image'] == name_path]['caption']\n","\n","    for key, value in labels.items():\n","        print(value)\n","\n","\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    image_input = transform(image).unsqueeze(0)\n","    caption = model.generate_caption(image_input)\n","\n","    plot_result(image, caption)\n"],"metadata":{"id":"Jk3RDPrhqkXr"},"execution_count":null,"outputs":[]}]}