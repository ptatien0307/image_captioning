{"cells":[{"cell_type":"markdown","metadata":{"id":"nNDmiaTFOamQ"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z-jdf2WJpph","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ec9fa92-07c9-4aa9-82d4-c526fe5b0b2d","executionInfo":{"status":"ok","timestamp":1707302685036,"user_tz":-420,"elapsed":3587,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/13dGpwyY-c5FPJTEacGkw8XNTkbGVWT2D/ImageCaptioning\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/ImageCaptioning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kU4gFEjlJxiD"},"outputs":[],"source":["import re\n","import os\n","import cv2\n","import math\n","import glob\n","import spacy\n","import random\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from PIL import Image\n","from tqdm import tqdm\n","import tensorflow as tf\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","\n","\n","import torch\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torch.nn import TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"GiBP0kLcqLe9"}},{"cell_type":"code","source":["def map_target(in_caption):\n","    out_caption = list()\n","    for caption5s in in_caption:\n","        temp5 = list()\n","        for cap in caption5s:\n","            out_cap = list()\n","            for idx in cap:\n","                if idx == 0:\n","                    break\n","                else:\n","                    out_cap.append(dataset.vocab.index2word[idx])\n","            temp5.append(out_cap)\n","        out_caption.append(temp5)\n","    return out_caption\n","\n","\n","def map_predict(in_caption):\n","    out_caption = list()\n","    for idx in in_caption:\n","        if idx == 2:\n","            break\n","        else:\n","            out_caption.append(dataset.vocab.index2word[idx])\n","    return out_caption"],"metadata":{"id":"-sxjNr40qNov"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZkXk51z8OcZi"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00_CvLY8FJp2"},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.index2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n","        self.word2index = {v: k for k, v in self.index2word.items()}\n","\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.index2word)\n","\n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","\n","    def build_vocab(self, sentence_list):\n","        frequencies = Counter()\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","\n","                #add the word to the vocab if it reaches minum frequecy threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.word2index[word] = idx\n","                    self.index2word[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [self.word2index[token] if token in self.word2index else self.word2index[\"<UNK>\"] for token in tokenized_text ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkcmwGqMC3o9"},"outputs":[],"source":["class ImageCaptioningDataset(Dataset):\n","    \"\"\"Image Captioning dataset\"\"\"\n","\n","    def __init__(self, csv_file, transform, freq_threshold=5):\n","        self.dataframe = pd.read_csv(csv_file)\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(\"dataset/Images\"))\n","        self.captions = self.dataframe['caption']\n","\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.captions.tolist())\n","\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        captions = self.captions[5 * idx: 5 * idx + 5].tolist()\n","        image_path = self.images[idx]\n","\n","        image = cv2.imread(f'dataset/Images/{image_path}')\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        caption_vec = []\n","        caption_vec.append(torch.full((50,), 0))\n","        for cap in captions:\n","            temp = self.vocab.numericalize(cap)\n","            caption_vec.append(torch.tensor(temp))\n","\n","        targets = pad_sequence(caption_vec, batch_first=True, padding_value=0)\n","\n","        return image, targets"]},{"cell_type":"code","source":["dataset = ImageCaptioningDataset(\n","                    csv_file=f\"dataset/captions.txt\",\n","                    transform=transforms.Compose([\n","                    transforms.ToTensor(),\n","                    transforms.Resize(232, antialias=True),\n","                    transforms.CenterCrop(224),\n","                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                        std=[0.229, 0.224, 0.225])]))\n","\n","\n","\n","loader = DataLoader(\n","                dataset=dataset,\n","                batch_size=16,\n","                num_workers=2)"],"metadata":{"id":"-4GrMRytqVM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Par-Inject"],"metadata":{"id":"W8JK72caFXlo"}},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"NfYZDGPmFZKq"}},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"vk0Qy6F2FbOL"}},{"cell_type":"code","source":["class Encoder(torch.nn.Module):\n","    def __init__(self, encoder_dim):\n","        super(Encoder, self).__init__()\n","        self.encoder_dim = encoder_dim\n","\n","        # Load pretrained model and remove last fc layer\n","        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-1]).to(device)\n","\n","        # Freeze layer\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # Add a linear layer add the end of model\n","        self.linear = torch.nn.Linear(2048, self.encoder_dim).to(device)\n","        self.drop = torch.nn.Dropout(0.3)\n","\n","    def forward(self, images):\n","        images = images.to(device)\n","\n","        # Forward pass\n","        features = self.model(images)                     # (batch_size, 2048, 1, 1)\n","        features = features.view(images.shape[0], 1, -1)  # (batch_size, 1, 2048)\n","        features = self.linear(self.drop(features))       # (batch_size, 1, 512)\n","        features = features.squeeze(1)                    # (batch_size, 512)\n","        return features"],"metadata":{"id":"8VUGjIMpFaEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"M5Ee2hW6FeKc"}},{"cell_type":"code","source":["class Decoder(torch.nn.Module):\n","    def __init__(self, vocab_size, embed_dim, encoder_dim, decoder_dim, num_layers):\n","        super(Decoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.encoder_dim = encoder_dim\n","        self.decoder_dim = decoder_dim\n","        self.num_layers = num_layers\n","\n","\n","\n","        # Embedding layer\n","        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n","\n","        # LSTM layer\n","        self.lstm = torch.nn.LSTM(input_size=embed_dim + encoder_dim,\n","                                  hidden_size=decoder_dim,\n","                                  bias=True,\n","                                  batch_first=True,\n","                                  num_layers=self.num_layers,\n","                                  bidirectional=False).to(device)\n","\n","        # Linear layer\n","        self.linear1 = torch.nn.Linear(decoder_dim, decoder_dim).to(device)\n","        self.linear2 = torch.nn.Linear(decoder_dim, vocab_size).to(device)\n","        self.drop = torch.nn.Dropout(0.3)\n","\n","    def init_hidden_state(self, features):\n","        hidden = torch.zeros(self.num_layers, features.size(0), self.decoder_dim).to(device)\n","        cell = torch.zeros(self.num_layers, features.size(0), self.decoder_dim).to(device)\n","        return hidden, cell\n","\n","    def forward_step(self, features, embed_words):\n","        # Init hidden state\n","        hidden_state, cell_state = self.init_hidden_state(features)\n","\n","        # Concat embedding and context vector\n","        features = features.unsqueeze(1)                             # (batch_size, feature_dim)\n","        features = features.repeat(1, embed_words.shape[1], 1)       # (batch_size, sequence_length, feature_dim)\n","        lstm_input = torch.cat((embed_words, features), dim=2)       # (batch_size, sequence_length, feature_dim + embed_dim)\n","\n","        # Forward pass\n","        output, (hn, cn) = self.lstm(lstm_input, (hidden_state, cell_state))\n","\n","        output = self.linear1(output)\n","        output = self.drop(output)\n","        output = self.linear2(output)\n","\n","        return output\n","\n","    def forward(self, features, sequences):\n","        # Embedding sequence\n","        sequence_length = len(sequences[0]) - 1\n","        sequences = sequences[:, :-1].to(device)\n","        embed_words = self.embedding(sequences)\n","        embed_words = embed_words.to(torch.float32)\n","\n","        output = self.forward_step(features, embed_words)\n","        return output\n","\n","\n","    def predict(self, features, max_length, vocab):\n","        # Embedding sequence\n","        words = torch.full((features.shape[0], 1), vocab.word2index['<SOS>']).to(device)\n","        embed_words = self.embedding(words)\n","        features = features.to(device)\n","\n","        predicted_captions = torch.zeros(features.shape[0], max_length)\n","\n","        for idx in range(max_length):\n","            # Predict word index\n","            output = self.forward_step(features, embed_words)[:, -1]\n","            predicted_word_idx = output.argmax(dim=1)\n","            predicted_captions[:, idx] = predicted_word_idx.unsqueeze(0)[:, :]\n","\n","            # Procedd with the next predicted word\n","            next_embed_word = self.embedding(predicted_word_idx).unsqueeze(0)\n","            next_embed_word = next_embed_word.permute(1, 0, 2)\n","            embed_words = torch.cat((embed_words, next_embed_word), dim=1)\n","\n","        return predicted_captions"],"metadata":{"id":"Ew71jJSwFeuz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Captioner"],"metadata":{"id":"0WrCmbQzFhgB"}},{"cell_type":"code","source":["class Captioner(torch.nn.Module):\n","    def __init__(self, vocab_size,  vocab, embed_dim, encoder_dim, decoder_dim, num_layers):\n","        super().__init__()\n","        self.encoder =  Encoder(encoder_dim)\n","        self.decoder = Decoder(vocab_size, embed_dim, encoder_dim, decoder_dim, num_layers)\n","        self.vocab = vocab\n","\n","    def forward(self, images, captions):\n","\n","        image_fv = self.encoder(images)\n","        output = self.decoder(image_fv, captions)\n","\n","        return output\n","\n","    def generate_caption(self, image, max_length=20):\n","        feature = self.encoder(image)\n","        predicted_caption = self.decoder.predict(feature, max_length, self.vocab)\n","\n","        return predicted_caption"],"metadata":{"id":"fuRPgeQgFisl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"2gbf1nM5FlEY"}},{"cell_type":"code","source":["def load_model(path):\n","    checkpoint = torch.load(path)\n","    model = Captioner(\n","        vocab_size=checkpoint['vocab_size'],\n","        vocab=checkpoint['vocab'],\n","        embed_dim=checkpoint['embed_dim'],\n","        encoder_dim=checkpoint['encoder_dim'],\n","        decoder_dim=checkpoint['decoder_dim'],\n","        num_layers=checkpoint['num_layers'],\n","    )\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    return model"],"metadata":{"id":"S-F1muELFmW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model(\"models/new-par_inject/model_best.pth\")\n","model.eval()\n","print(\"Load model successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAX3jLJdI-cQ","executionInfo":{"status":"ok","timestamp":1707302688881,"user_tz":-420,"elapsed":929,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}},"outputId":"60483ebf-691e-4179-9e46-2588d587a64d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Load model successfully\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","    list_of_references = []\n","    hypotheses = []\n","    bleu_score = []\n","    for idx, (images, targets) in tqdm(enumerate(iter(loader))):\n","        images, targets = images.to(device), targets[:, 1:, :].tolist()\n","\n","\n","        mapped_targets = map_target(targets)\n","        list_of_references.extend(mapped_targets)\n","\n","        predicted_captions = model.generate_caption(images).tolist()\n","        predicted_captions = list(map(map_predict, predicted_captions))\n","\n","        hypotheses.extend(predicted_captions)\n","        score = corpus_bleu(list_of_references, hypotheses)\n","        bleu_score.append(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBarQfUpJBrF","executionInfo":{"status":"ok","timestamp":1707303331823,"user_tz":-420,"elapsed":642950,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}},"outputId":"c2919c0d-3bae-4355-cc63-28a88b0a30a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["506it [10:42,  1.27s/it]\n"]}]},{"cell_type":"code","source":["sum(bleu_score) / len(bleu_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qe8pCsBKJC89","executionInfo":{"status":"ok","timestamp":1707303331824,"user_tz":-420,"elapsed":40,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}},"outputId":"97373bbe-5f32-41d8-f77a-391cadfd8624"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.39452743009536984"]},"metadata":{},"execution_count":87}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["GiBP0kLcqLe9","vk0Qy6F2FbOL","0WrCmbQzFhgB"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}