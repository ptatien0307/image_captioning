{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDmiaTFOamQ"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Z-jdf2WJpph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd1263c-96e1-4db3-b59e-8b5192645566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ImageCaptioning\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ImageCaptioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kU4gFEjlJxiD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import spacy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkXk51z8OcZi"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "00_CvLY8FJp2"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.index2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.word2index[word] = idx\n",
        "                    self.index2word[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [self.word2index[token] if token in self.word2index else self.word2index[\"<UNK>\"] for token in tokenized_text ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bkcmwGqMC3o9"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningDataset(Dataset):\n",
        "    \"\"\"Image Captioning dataset\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, transform, freq_threshold=5):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(\"dataset/Images\"))\n",
        "        self.captions = self.dataframe['caption']\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        captions = self.captions[5 * idx: 5 * idx + 5].tolist()\n",
        "        image_path = self.images[idx]\n",
        "\n",
        "        image = cv2.imread(f'dataset/Images/{image_path}')\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_vec = []\n",
        "        caption_vec.append(torch.full((50,), 0))\n",
        "        for cap in captions:\n",
        "            temp = self.vocab.numericalize(cap)\n",
        "            caption_vec.append(torch.tensor(temp))\n",
        "\n",
        "        targets = pad_sequence(caption_vec, batch_first=True, padding_value=0)\n",
        "\n",
        "        return image, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNig4R6wR-Ko"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_y6uEWHdCjT"
      },
      "source": [
        "## Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cytNYFuFYpUR"
      },
      "outputs": [],
      "source": [
        "class ImageFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Load pretrained model and remove last fc layer\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-2]).to(device)\n",
        "\n",
        "        # Freeze layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Preprocess images\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = self.model(images)                                       # (batch_size, 2048, 7, 7)\n",
        "        features = features.permute(0, 2, 3, 1)                             # (batch_size, 7, 7, 2048)\n",
        "        features = features.view(features.size(0), -1, features.size(-1))   # (batch_size, 49, 2048)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbI3aw5_XwNj"
      },
      "source": [
        "## Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DG1UDB0LXyao"
      },
      "outputs": [],
      "source": [
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, attention_dim, encoder_dim, decoder_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention_dim = attention_dim\n",
        "        self.W_layer = torch.nn.Linear(decoder_dim, attention_dim).to(device)\n",
        "        self.U_layer = torch.nn.Linear(encoder_dim, attention_dim).to(device)\n",
        "        self.V_layer = torch.nn.Linear(attention_dim, 1).to(device)\n",
        "\n",
        "    def forward(self, keys, query):\n",
        "        U = self.U_layer(keys)     # (batch_size, num_layers, attention_dim)\n",
        "        W = self.W_layer(query) # (batch_size, attention_dim)\n",
        "\n",
        "        combined = torch.tanh(U + W.unsqueeze(1)) # (batch_size, num_layers, attention_dim)\n",
        "        score = self.V_layer(combined)  # (batch_size, num_layers, 1)\n",
        "        score = score.squeeze(2) # (batch_size, num_layers)\n",
        "\n",
        "        weights = F.softmax(score, dim=1)    # (batch_size, num_layers)\n",
        "\n",
        "        context = keys * weights.unsqueeze(2) # (batch_size, num_layers, feature_dim)\n",
        "        context = context.sum(dim=1)   # (batch_size, feature_dim)\n",
        "        return context, weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsk7dsFSqbc"
      },
      "source": [
        "## Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BLXEt0MNsc5c"
      },
      "outputs": [],
      "source": [
        "class TextFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3):\n",
        "        super(TextFeatureExtractor, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = torch.nn.LSTMCell(input_size=embed_dim + encoder_dim,\n",
        "                                      hidden_size=decoder_dim, bias=True).to(device)\n",
        "\n",
        "        # Linear layer\n",
        "        self.fcn = torch.nn.Linear(decoder_dim, self.vocab_size).to(device)\n",
        "        self.drop = torch.nn.Dropout(drop_prob)\n",
        "\n",
        "        # Attention layer\n",
        "        self.init_h = torch.nn.Linear(encoder_dim, decoder_dim).to(device)\n",
        "        self.init_c = torch.nn.Linear(encoder_dim, decoder_dim).to(device)\n",
        "        self.attention = Attention(attention_dim, encoder_dim, decoder_dim)\n",
        "\n",
        "    def init_hidden_state(self, features):\n",
        "        mean_features = features.mean(dim=1)\n",
        "        h = self.init_h(mean_features)\n",
        "        c = self.init_c(mean_features)\n",
        "        return h, c\n",
        "\n",
        "    def forward_step(self, embed_word, features, hidden_state, cell_state):\n",
        "        # Computation between features and hidden state to create a context vector\n",
        "        context, attn_weight = self.attention(features, hidden_state)\n",
        "\n",
        "        # Compute feature vector of input text\n",
        "        lstm_input = torch.cat((embed_word, context), dim=1)\n",
        "\n",
        "        hidden_state, cell_state = self.lstm(lstm_input, (hidden_state, cell_state))\n",
        "\n",
        "        # Predicted vector\n",
        "        output = self.fcn(self.drop(hidden_state))\n",
        "\n",
        "        return output, hidden_state, attn_weight\n",
        "\n",
        "    def forward(self, features, sequences):\n",
        "        # Sequence\n",
        "        sequence_length = len(sequences[0]) - 1\n",
        "        sequences = sequences.to(device)\n",
        "\n",
        "        # Prediction store\n",
        "        preds = torch.zeros(sequences.shape[0], sequence_length, self.vocab_size).to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(sequences)\n",
        "        embeds = embeds.to(torch.float32)\n",
        "\n",
        "        # Init hidden state\n",
        "        hidden_state, cell_state = self.init_hidden_state(features)\n",
        "\n",
        "        # Forward pass\n",
        "        for idx in range(sequence_length):\n",
        "            embed_word = embeds[:, idx]\n",
        "\n",
        "            # Predicted vector\n",
        "            output, hidden_state, _ = self.forward_step(embed_word, features, hidden_state, cell_state)\n",
        "\n",
        "            # Store output\n",
        "            preds[:, idx] = output\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def predict(self, feature, max_length, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.tensor(vocab.word2index['<SOS>']).view(1, -1).to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        captions = []\n",
        "        attention = []\n",
        "        hidden_state, cell_state = self.init_hidden_state(feature)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden_state, attn_weight = self.forward_step(embed_word, feature, hidden_state, cell_state)\n",
        "            attention.append(attn_weight.cpu().detach().numpy())\n",
        "\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "\n",
        "            # End if <EOS> appears\n",
        "            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        # Convert the vocab idx to words and return sentence\n",
        "        return ' '.join([vocab.index2word[idx] for idx in captions]), attention\n",
        "\n",
        "\n",
        "    def predict_batch(self, feature, max_length, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.full((feature.shape[0], 1), vocab.word2index['<SOS>']).to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "        predicted_captions = torch.zeros(20, feature.shape[0])\n",
        "        hidden_state, cell_state = self.init_hidden_state(feature)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden_state, attn_weight = self.forward_step(embed_word, feature, hidden_state, cell_state)\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            predicted_captions[idx, :] = predicted_word_idx.unsqueeze(0)[:, :]\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(1))\n",
        "        predicted_captions = predicted_captions.permute(1, 0)\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO_HFiZzdGNH"
      },
      "source": [
        "## Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QpXn1pUBzmuf"
      },
      "outputs": [],
      "source": [
        "class Captioner(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, attention_dim, encoder_dim, decoder_dim, vocab):\n",
        "        super(Captioner, self).__init__()\n",
        "        self.image_encoder =  ImageFeatureExtractor()\n",
        "        self.text_decoder = TextFeatureExtractor(vocab_size, embed_dim, attention_dim,\n",
        "                                                 encoder_dim, decoder_dim)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        features = self.image_encoder(images)\n",
        "        output = self.text_decoder(features, captions)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_caption(self, image, max_length=20):\n",
        "        image = image.to(device)\n",
        "        feature = self.image_encoder(image)\n",
        "        predicted_caption, attn_weights = self.text_decoder.predict(feature, max_length, self.vocab)\n",
        "\n",
        "        return predicted_caption, attn_weights\n",
        "\n",
        "    def generate_caption_batch(self, images, max_length=20):\n",
        "        images = images.to(device)\n",
        "        features = self.image_encoder(images)\n",
        "        predicted_captions = self.text_decoder.predict_batch(features, max_length, self.vocab)\n",
        "\n",
        "        return predicted_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm2wZt8MG_0Q"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BNTUnGL9HBHA"
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "\n",
        "    model = Captioner(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        embed_dim=checkpoint['embed_dim'],\n",
        "        attention_dim=checkpoint['attention_dim'],\n",
        "        encoder_dim=checkpoint['encoder_dim'],\n",
        "        decoder_dim=checkpoint['decoder_dim'],\n",
        "        vocab=checkpoint['vocab']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"models/bahdanau_attn/model_best.pth\")\n",
        "model.eval()\n",
        "dataset = ImageCaptioningDataset(\n",
        "                    csv_file=f\"dataset/captions.txt\",\n",
        "                    transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize(232, antialias=True),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])]))\n",
        "\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "                dataset=dataset,\n",
        "                batch_size=32,\n",
        "                num_workers=2)\n"
      ],
      "metadata": {
        "id": "uWBOfeoSfcfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8c45c4-7299-460b-9f12-1bd9b342d903"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_target(in_caption):\n",
        "    out_caption = list()\n",
        "    for caption5s in in_caption:\n",
        "        temp5 = list()\n",
        "        for cap in caption5s:\n",
        "            out_cap = list()\n",
        "            for idx in cap:\n",
        "                if idx == 0:\n",
        "                    break\n",
        "                else:\n",
        "                    out_cap.append(dataset.vocab.index2word[idx])\n",
        "            temp5.append(out_cap)\n",
        "        out_caption.append(temp5)\n",
        "    return out_caption\n",
        "\n",
        "\n",
        "def map_predict(in_caption):\n",
        "    out_caption = list()\n",
        "    for idx in in_caption:\n",
        "        if idx == 2:\n",
        "            break\n",
        "        else:\n",
        "            out_caption.append(dataset.vocab.index2word[idx])\n",
        "    return out_caption"
      ],
      "metadata": {
        "id": "i_NfjZZezs5j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7Yv-MYHGf5",
        "outputId": "c404cde9-7ea5-4882-d812-79ef4c31bee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:12,  5.16s/it]"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    list_of_references = []\n",
        "    hypotheses = []\n",
        "    bleu_score = []\n",
        "    for idx, (image, target) in tqdm(enumerate(iter(loader))):\n",
        "        image, target = image.to(device), target[:, 1:, :].tolist()\n",
        "\n",
        "\n",
        "        mapped_target = map_target(target)\n",
        "        list_of_references.extend(mapped_target)\n",
        "\n",
        "        predicted_captions = model.generate_caption_batch(image).tolist()\n",
        "        predicted_captions= list(map(map_predict, predicted_captions))\n",
        "\n",
        "        hypotheses.extend(predicted_captions)\n",
        "        score = corpus_bleu(list_of_references, hypotheses)\n",
        "        bleu_score.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(bleu_score) / len(bleu_score)"
      ],
      "metadata": {
        "id": "AQZkFERZ9K4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGa4lz7oKgoO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nNDmiaTFOamQ",
        "ZkXk51z8OcZi",
        "gNig4R6wR-Ko",
        "2_y6uEWHdCjT",
        "WbI3aw5_XwNj"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}