{"cells":[{"cell_type":"markdown","metadata":{"id":"nNDmiaTFOamQ"},"source":["# Libraries"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2521,"status":"ok","timestamp":1704265545572,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"6Z-jdf2WJpph","outputId":"cc8cf9f4-f8b9-4c5d-ea53-9f0e319c9a38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/.shortcut-targets-by-id/13dGpwyY-c5FPJTEacGkw8XNTkbGVWT2D/ImageCaptioning\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd /content/gdrive/MyDrive/ImageCaptioning"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704265545572,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"kU4gFEjlJxiD"},"outputs":[],"source":["import re\n","import cv2\n","import glob\n","import spacy\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from PIL import Image\n","from numpy import array\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from collections import Counter\n","import torch\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision import transforms\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25234,"status":"ok","timestamp":1704265570801,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"Y9h9t6cYFhoO","outputId":"61ba808a-2484-4703-ad22-da39dc799eb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-03 07:05:50.123641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-03 07:05:50.123756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-03 07:05:50.126339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-03 07:05:52.168610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"]}],"source":["!python -m spacy download en_core_web_sm\n","!pip install tensorboard"]},{"cell_type":"markdown","metadata":{"id":"ZkXk51z8OcZi"},"source":["# Dataset"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":1809,"status":"ok","timestamp":1704265572595,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"l9yrcE4SFUFJ"},"outputs":[],"source":["spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704265572596,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"00_CvLY8FJp2"},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self,freq_threshold):\n","        # Setting the pre-reserved tokens int to string tokens\n","        self.index2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n","\n","        # String to int tokens\n","        # Tts reverse dict self.index2word\n","        self.word2index = {v: k for k, v in self.index2word.items()}\n","\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.index2word)\n","\n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","\n","    def build_vocab(self, sentence_list):\n","        frequencies = Counter()\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","\n","                #add the word to the vocab if it reaches minum frequecy threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.word2index[word] = idx\n","                    self.index2word[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self,text):\n","        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [self.word2index[token] if token in self.word2index else self.word2index[\"<UNK>\"] for token in tokenized_text ]"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704265572596,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"bkcmwGqMC3o9"},"outputs":[],"source":["class ImageCaptioningDataset(Dataset):\n","    \"\"\"Image Captioning dataset\"\"\"\n","\n","    def __init__(self, csv_file, transform, freq_threshold=5):\n","        self.dataframe = pd.read_csv(csv_file)\n","        self.transform = transform\n","\n","        self.images = self.dataframe['image']\n","        self.captions = self.dataframe['caption']\n","\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.captions.tolist())\n","\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        caption = self.captions[idx]\n","        image_path = self.images[idx]\n","\n","        image = cv2.imread(f'dataset/Images/{image_path}')\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        caption_vec = []\n","        caption_vec += [self.vocab.word2index[\"<SOS>\"]]\n","        caption_vec += self.vocab.numericalize(caption)\n","        caption_vec += [self.vocab.word2index[\"<EOS>\"]]\n","\n","        return image, torch.tensor(caption_vec)"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704265572596,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"erFkeykaOV5a"},"outputs":[],"source":["class CapsCollate:\n","    def __init__(self, pad_idx, batch_first=False):\n","        self.pad_idx = pad_idx\n","        self.batch_first = batch_first\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n","        return imgs, targets"]},{"cell_type":"markdown","metadata":{"id":"gNig4R6wR-Ko"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"2_y6uEWHdCjT"},"source":["## Image"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704265572596,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"cytNYFuFYpUR"},"outputs":[],"source":["class ImageFeatureExtractor(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Load pretrained model and remove last fc layer\n","        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-1]).to(device)\n","\n","        # Freeze layer\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # Add a linear layer add the end of model\n","        self.linear = torch.nn.Linear(2048, 512).to(device)\n","        self.drop = torch.nn.Dropout(0.3)\n","\n","    def forward(self, images):\n","        # Preprocess images\n","        images = images.to(device)\n","\n","        # Forward pass\n","        feature = self.model(images)                   # (batch_size, 2048, 1, 1)\n","        feature = feature.view(images.shape[0], 1, -1) # (batch_size, 1, 2048)\n","        feature = self.drop(feature)\n","        output = self.linear(feature).squeeze(1)       # (batch_size, 512)\n","\n","        # Return output\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"wpsk7dsFSqbc"},"source":["## Text"]},{"cell_type":"code","source":["class TextFeatureExtractor(torch.nn.Module):\n","    def __init__(self, vocab_size, embed_dim):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","\n","        # Embedding layer\n","        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n","\n","        # LSTM layer\n","        self.lstm = torch.nn.LSTMCell(input_size=embed_dim, hidden_size=512).to(device)\n","\n","        # Linear layer\n","        self.decoder1 = torch.nn.Linear(1024, 512).to(device)\n","        self.decoder2 = torch.nn.Linear(512, self.vocab_size).to(device)\n","        self.drop = torch.nn.Dropout(0.3)\n","\n","    def forward(self, features, sequences):\n","\n","        sequence_length = len(sequences[0]) - 1\n","        preds = torch.zeros(sequences.shape[0], sequence_length, self.vocab_size)\n","\n","        sequences = sequences.to(device)\n","        preds = preds.to(device)\n","\n","        # Embedding sequence\n","        embeds = self.embedding(sequences)\n","        embeds = embeds.to(torch.float32)\n","\n","        # Forward pass\n","        for idx in range(sequence_length):\n","            # Compute feature vector of input text\n","            embed_word = embeds[:, idx]\n","            h, c = self.lstm(embed_word)\n","            # Concat fe of image and text\n","            concat = torch.cat((features, h), 1)\n","\n","            # Pass concat fe to decoder\n","            decoded = self.decoder1(concat)\n","            decoded = self.drop(decoded)\n","            output = self.decoder2(decoded)\n","\n","            # Predicted vector\n","            preds[:, idx] = output\n","\n","        return preds\n","\n","    def predict(self, feature, max_length=20, vocab=None):\n","        # Starting input\n","        word = torch.tensor(vocab.word2index['<SOS>']).view(1, -1)\n","        word = word.to(device)\n","        feature = feature.to(device)\n","\n","        # Embedding sequence\n","        embeds = self.embedding(word)\n","\n","        captions = []\n","\n","        for i in range(max_length):\n","\n","            # Compute feature vector of input text\n","            embed_word = embeds[:, idx]\n","            hidden_state, cell_state = self.lstm(embed_word)\n","\n","            # Concat fe of image and text\n","            concat = torch.cat((feature, hidden_state), 1)\n","\n","            # Pass concat fe to decoder\n","            decoded = self.decoder1(concat)\n","            decoded = self.drop(decoded)\n","            output = self.decoder2(decoded)\n","\n","            # Predict word index\n","            predicted_word_idx = output.argmax(dim=1)\n","            captions.append(predicted_word_idx.item())\n","\n","            # End if <EOS> appears\n","            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n","                break\n","\n","            # Send generated word as the next caption\n","            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n","\n","        # Convert the vocab idx to words and return sentence\n","        return ' '.join([vocab.index2word[idx] for idx in captions])"],"metadata":{"id":"wiws1A5y3jos","executionInfo":{"status":"ok","timestamp":1704265572596,"user_tz":-420,"elapsed":6,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zO_HFiZzdGNH"},"source":["## Captioner"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704265572596,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"QpXn1pUBzmuf"},"outputs":[],"source":["class Captioner(torch.nn.Module):\n","    def __init__(self, vocab_size, embed_dim, vocab):\n","        super().__init__()\n","        self.image_fe =  ImageFeatureExtractor()\n","        self.text_fe = TextFeatureExtractor(vocab_size, embed_dim)\n","        self.vocab = vocab\n","\n","    def forward(self, images, captions):\n","\n","        image_fv = self.image_fe(images)\n","        output = self.text_fe(image_fv, captions)\n","\n","        return output\n","\n","    def generate_caption(self, image, max_length=20):\n","        feature = self.image_fe(image)\n","        predicted_caption = self.text_fe.predict(feature, max_length, self.vocab)\n","\n","        return predicted_caption\n"]},{"cell_type":"markdown","metadata":{"id":"pkALufNkX9ed"},"source":["# Train"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1117,"status":"ok","timestamp":1704265573708,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"},"user_tz":-420},"id":"3tXOgwscJBla","outputId":"7e219c8a-b7e5-4a3a-91b5-1a72f4a9bfe1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=-319.3532>"]},"metadata":{},"execution_count":52}],"source":["# Warmp up GPU and CPU\n","def cpu():\n","  with tf.device('/cpu:0'):\n","    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n","    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n","    return tf.math.reduce_sum(net_cpu)\n","\n","def gpu():\n","  with tf.device('/device:GPU:0'):\n","    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n","    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n","    return tf.math.reduce_sum(net_gpu)\n","\n","cpu()\n","gpu()"]},{"cell_type":"code","source":["class Trainer():\n","    def __init__(self):\n","        # Dataset\n","        self.dataset =  ImageCaptioningDataset(\n","            csv_file=\"dataset/captions.txt\",\n","            transform=transforms.Compose([\n","                    transforms.ToTensor(),\n","                    transforms.Resize((224, 224), antialias=True),\n","                    transforms.CenterCrop(224),\n","                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                        std=[0.229, 0.224, 0.225])\n","                    ])\n","            )\n","\n","\n","        # Hyperparameters\n","        self.vocab_size = len(self.dataset.vocab)\n","        self.vocab = self.dataset.vocab\n","        self.embed_dim = 300\n","        self.num_epochs = 50\n","        self.batch_size = 4\n","\n","        # Parameters\n","        self.writer = SummaryWriter('runs')\n","\n","        # Dataloader\n","        self.data_loader = DataLoader(\n","            dataset=self.dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            collate_fn=CapsCollate(pad_idx=self.dataset.vocab.word2index[\"<PAD>\"], batch_first=True)\n","        )\n","    def train(self,resume=False):\n","        # Init model, optimizer, criterion\n","        model = Captioner(\n","            vocab_size=self.vocab_size,\n","            embed_dim=self.embed_dim,\n","            vocab=self.dataset.vocab\n","        )\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.dataset.vocab.word2index[\"<PAD>\"])\n","\n","        # Starting epoch\n","        start_epoch = 0\n","        min_loss = 9999\n","\n","        if resume:\n","            # Load model and optimizer state\n","            model_state, optimizer_state, prev_epoch, prev_loss = self.load_model()\n","            model.load_state_dict(model_state)\n","            optimizer.load_state_dict(optimizer_state)\n","\n","            # Starting epoch\n","            start_epoch = prev_epoch\n","            min_loss = prev_loss\n","\n","        for epoch in range(start_epoch + 1, self.num_epochs + 1):\n","            epoch_loss = []\n","            train_pbar = tqdm(enumerate(iter(self.data_loader)), position=0, leave=True)\n","            for idx, (image, captions) in train_pbar:\n","                image, captions = image.to(device), captions.to(device)\n","\n","                # Zero the gradients\n","                optimizer.zero_grad()\n","\n","                # Feed forward\n","                outputs = model(image, captions)\n","\n","                # Calculate the batch loss\n","                targets = captions[:, 1:]\n","                loss = criterion(outputs.view(-1, self.vocab_size), targets.reshape(-1))\n","                epoch_loss.append(loss.item())\n","\n","                # Backward pass\n","                loss.backward()\n","\n","                # Update the parameters in the optimizer\n","                optimizer.step()\n","\n","                # Show progess bar with loss per batch\n","                train_pbar.set_postfix_str(f\"Loss: {sum(epoch_loss) / len(epoch_loss):0.4f}\")\n","                break\n","\n","            # Compute average loss per epoch\n","            avg_epoch_loss = sum(epoch_loss) / len(epoch_loss)\n","            self.writer.add_scalar(\"Loss/Train\", avg_epoch_loss, epoch + 1)\n","\n","            # Save model\n","            if avg_epoch_loss < min_loss:\n","                self.save_model(model, optimizer, epoch, avg_epoch_loss)\n","            break\n","        self.writer.close()\n","\n","    def save_model(self, model, optimizer, epoch, loss):\n","        model_state = {\n","            'epoch': epoch,\n","            'loss': loss,\n","            'embed_dim': self.embed_dim,\n","            'vocab_size': self.vocab_size,\n","            'vocab': self.vocab,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()\n","        }\n","\n","        torch.save(model_state, f'models/merge/model_{epoch}_{loss:.4f}.pth')\n","        torch.save(model_state, f'models/merge/model_best.pth')\n","\n","    def load_model(self):\n","        path = glob.glob(\"models/merge/*.pth\")[-2]\n","        checkpoint = torch.load(path)\n","\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        embed_dim = checkpoint['embed_dim']\n","        vocab_size = checkpoint['vocab_size']\n","        vocab = checkpoint['vocab']\n","\n","        model_state = checkpoint['model_state_dict']\n","        optimizer_state = checkpoint['optimizer_state_dict']\n","\n","        return model_state, optimizer_state, epoch, loss"],"metadata":{"id":"whvNViSQFCAC","executionInfo":{"status":"ok","timestamp":1704267764954,"user_tz":-420,"elapsed":326,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer()\n","trainer.train(resume=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PH6UQVe_PLA","executionInfo":{"status":"ok","timestamp":1704267791806,"user_tz":-420,"elapsed":7718,"user":{"displayName":"Tiên Phạm Trần Anh","userId":"02696921126809248619"}},"outputId":"a0e30bed-645a-4dca-a1ef-f24836d414b3"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:03, ?it/s, Loss: 7.8404]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk7siPoZUb0R"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir runs"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["nNDmiaTFOamQ","ZkXk51z8OcZi","gNig4R6wR-Ko"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
