{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YexcTDe-FWve"
      },
      "outputs": [],
      "source": [
        "# bahdanau             - 0.5530967579684627\n",
        "# luong                - 0.5891869092912313\n",
        "# par-inject           - 0.34457920062888253\n",
        "# par-inject 4-lstm    - 0.39452743009536984\n",
        "# init-inject          - 0.27340657084194314\n",
        "# transformer          - 0.37336743809031714"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDmiaTFOamQ"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z-jdf2WJpph",
        "outputId": "9273c641-9a33-45fc-d784-ab488e88c68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/13dGpwyY-c5FPJTEacGkw8XNTkbGVWT2D/ImageCaptioning\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ImageCaptioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU4gFEjlJxiD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import glob\n",
        "import spacy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBP0kLcqLe9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sxjNr40qNov"
      },
      "outputs": [],
      "source": [
        "def map_target(in_caption):\n",
        "    out_caption = list()\n",
        "    for caption5s in in_caption:\n",
        "        temp5 = list()\n",
        "        for cap in caption5s:\n",
        "            out_cap = list()\n",
        "            for idx in cap:\n",
        "                if idx == 0:\n",
        "                    break\n",
        "                else:\n",
        "                    out_cap.append(dataset.vocab.index2word[idx])\n",
        "            temp5.append(out_cap)\n",
        "        out_caption.append(temp5)\n",
        "    return out_caption\n",
        "\n",
        "\n",
        "def map_predict(in_caption):\n",
        "    out_caption = list()\n",
        "    for idx in in_caption:\n",
        "        if idx == 2:\n",
        "            break\n",
        "        else:\n",
        "            out_caption.append(dataset.vocab.index2word[idx])\n",
        "    return out_caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkXk51z8OcZi"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00_CvLY8FJp2"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.index2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.word2index[word] = idx\n",
        "                    self.index2word[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [self.word2index[token] if token in self.word2index else self.word2index[\"<UNK>\"] for token in tokenized_text ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkcmwGqMC3o9"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningDataset(Dataset):\n",
        "    \"\"\"Image Captioning dataset\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, transform, freq_threshold=5):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(\"dataset/Images\"))\n",
        "        self.captions = self.dataframe['caption']\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        captions = self.captions[5 * idx: 5 * idx + 5].tolist()\n",
        "        image_path = self.images[idx]\n",
        "\n",
        "        image = cv2.imread(f'dataset/Images/{image_path}')\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_vec = []\n",
        "        caption_vec.append(torch.full((50,), 0))\n",
        "        for cap in captions:\n",
        "            temp = self.vocab.numericalize(cap)\n",
        "            caption_vec.append(torch.tensor(temp))\n",
        "\n",
        "        targets = pad_sequence(caption_vec, batch_first=True, padding_value=0)\n",
        "\n",
        "        return image, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4GrMRytqVM2"
      },
      "outputs": [],
      "source": [
        "dataset = ImageCaptioningDataset(\n",
        "                    csv_file=f\"dataset/captions.txt\",\n",
        "                    transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize(232, antialias=True),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])]))\n",
        "\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "                dataset=dataset,\n",
        "                batch_size=32,\n",
        "                num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APrvh4KbLunt"
      },
      "source": [
        "# Bahdanau Atttention and Luong Atttention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNig4R6wR-Ko"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_y6uEWHdCjT"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cytNYFuFYpUR"
      },
      "outputs": [],
      "source": [
        "class ImageFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Load pretrained model and remove last fc layer\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-2]).to(device)\n",
        "\n",
        "        # Freeze layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Preprocess images\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = self.model(images)                                       # (batch_size, 2048, 7, 7)\n",
        "        features = features.permute(0, 2, 3, 1)                             # (batch_size, 7, 7, 2048)\n",
        "        features = features.view(features.size(0), -1, features.size(-1))   # (batch_size, 49, 2048)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbI3aw5_XwNj"
      },
      "source": [
        "### Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG1UDB0LXyao"
      },
      "outputs": [],
      "source": [
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, attention_dim, encoder_dim, decoder_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention_dim = attention_dim\n",
        "        self.W_layer = torch.nn.Linear(decoder_dim, attention_dim).to(device)\n",
        "        self.U_layer = torch.nn.Linear(encoder_dim, attention_dim).to(device)\n",
        "        self.V_layer = torch.nn.Linear(attention_dim, 1).to(device)\n",
        "\n",
        "    def forward(self, keys, query):\n",
        "        U = self.U_layer(keys)     # (batch_size, num_layers, attention_dim)\n",
        "        W = self.W_layer(query) # (batch_size, attention_dim)\n",
        "\n",
        "        combined = torch.tanh(U + W.unsqueeze(1)) # (batch_size, num_layers, attention_dim)\n",
        "        score = self.V_layer(combined)  # (batch_size, num_layers, 1)\n",
        "        score = score.squeeze(2) # (batch_size, num_layers)\n",
        "\n",
        "        weights = F.softmax(score, dim=1)    # (batch_size, num_layers)\n",
        "\n",
        "        context = keys * weights.unsqueeze(2) # (batch_size, num_layers, feature_dim)\n",
        "        context = context.sum(dim=1)   # (batch_size, feature_dim)\n",
        "        return context, weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsk7dsFSqbc"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLXEt0MNsc5c"
      },
      "outputs": [],
      "source": [
        "class TextFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3):\n",
        "        super(TextFeatureExtractor, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = torch.nn.LSTMCell(input_size=embed_dim + encoder_dim,\n",
        "                                      hidden_size=decoder_dim, bias=True).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # Attention layer\n",
        "        self.init_h = torch.nn.Linear(encoder_dim, decoder_dim).to(device)\n",
        "        self.init_c = torch.nn.Linear(encoder_dim, decoder_dim).to(device)\n",
        "        self.attention = Attention(attention_dim, encoder_dim, decoder_dim)\n",
        "        self.attention_type = 'luong'\n",
        "\n",
        "        if self.attention_type == 'bahdanau':\n",
        "            self.fcn = torch.nn.Linear(decoder_dim, self.vocab_size).to(device)\n",
        "        elif self.attention_type == 'luong':\n",
        "            self.fcn = torch.nn.Linear(decoder_dim + encoder_dim, self.vocab_size).to(device)\n",
        "\n",
        "        self.drop = torch.nn.Dropout(drop_prob)\n",
        "\n",
        "    def init_hidden_state(self, features):\n",
        "        mean_features = features.mean(dim=1)\n",
        "        h = self.init_h(mean_features)\n",
        "        c = self.init_c(mean_features)\n",
        "        return h, c\n",
        "\n",
        "    def forward_step(self, embed_word, features, hidden_state, cell_state):\n",
        "        # Computation between features and hidden state to create a context vector\n",
        "        context, attn_weight = self.attention(features, hidden_state)\n",
        "\n",
        "        # Compute feature vector of input text\n",
        "        lstm_input = torch.cat((embed_word, context), dim=1)\n",
        "\n",
        "        hidden_state, cell_state = self.lstm(lstm_input, (hidden_state, cell_state))\n",
        "\n",
        "        # Predicted vector\n",
        "        output = None\n",
        "        if self.attention_type == 'bahdanau':\n",
        "            output = self.fcn(self.drop(hidden_state))\n",
        "        elif self.attention_type == 'luong':\n",
        "            input_linear = torch.cat((hidden_state, context), dim=1)\n",
        "            output = self.fcn(self.drop(input_linear))\n",
        "\n",
        "        return output, hidden_state, cell_state, attn_weight\n",
        "\n",
        "    def forward(self, features, sequences):\n",
        "        # Sequence\n",
        "        sequence_length = len(sequences[0]) - 1\n",
        "        sequences = sequences.to(device)\n",
        "\n",
        "        # Prediction store\n",
        "        preds = torch.zeros(sequences.shape[0], sequence_length, self.vocab_size).to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(sequences)\n",
        "        embeds = embeds.to(torch.float32)\n",
        "\n",
        "        # Init hidden state\n",
        "        hidden_state, cell_state = self.init_hidden_state(features)\n",
        "\n",
        "        # Forward pass\n",
        "        for idx in range(sequence_length):\n",
        "            embed_word = embeds[:, idx]\n",
        "\n",
        "            # Predicted vector\n",
        "            output, hidden_state, cell_state, _ = self.forward_step(embed_word, features, hidden_state, cell_state)\n",
        "\n",
        "            # Store output\n",
        "            preds[:, idx] = output\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def predict(self, feature, max_length, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.tensor(vocab.word2index['<SOS>']).view(1, -1).to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        captions = []\n",
        "        attention = []\n",
        "        hidden_state, cell_state = self.init_hidden_state(feature)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden_state, cell_state, attn_weight = self.forward_step(embed_word, feature, hidden_state, cell_state)\n",
        "            attention.append(attn_weight.cpu().detach().numpy())\n",
        "\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "\n",
        "            # End if <EOS> appears\n",
        "            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        # Convert the vocab idx to words and return sentence\n",
        "        return ' '.join([vocab.index2word[idx] for idx in captions]), attention\n",
        "\n",
        "\n",
        "    def predict_batch(self, feature, max_length, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.full((feature.shape[0], 1), vocab.word2index['<SOS>']).to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "        predicted_captions = torch.zeros(max_length, feature.shape[0])\n",
        "        hidden_state, cell_state = self.init_hidden_state(feature)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden_state, cell_state, attn_weight = self.forward_step(embed_word, feature, hidden_state, cell_state)\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            predicted_captions[idx, :] = predicted_word_idx.unsqueeze(0)[:, :]\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(1))\n",
        "        predicted_captions = predicted_captions.permute(1, 0)\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO_HFiZzdGNH"
      },
      "source": [
        "### Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpXn1pUBzmuf"
      },
      "outputs": [],
      "source": [
        "class Captioner(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, attention_dim, encoder_dim, decoder_dim, vocab):\n",
        "        super(Captioner, self).__init__()\n",
        "        self.image_encoder =  ImageFeatureExtractor()\n",
        "        self.text_decoder = TextFeatureExtractor(vocab_size, embed_dim, attention_dim,\n",
        "                                                 encoder_dim, decoder_dim)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        features = self.image_encoder(images)\n",
        "        output = self.text_decoder(features, captions)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_caption(self, image, max_length=20):\n",
        "        image = image.to(device)\n",
        "        feature = self.image_encoder(image)\n",
        "        predicted_caption, attn_weights = self.text_decoder.predict(feature, max_length, self.vocab)\n",
        "\n",
        "        return predicted_caption, attn_weights\n",
        "\n",
        "    def generate_caption_batch(self, images, max_length=20):\n",
        "        images = images.to(device)\n",
        "        features = self.image_encoder(images)\n",
        "        predicted_captions = self.text_decoder.predict_batch(features, max_length, self.vocab)\n",
        "\n",
        "        return predicted_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm2wZt8MG_0Q"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNTUnGL9HBHA"
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "\n",
        "    model = Captioner(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        embed_dim=checkpoint['embed_dim'],\n",
        "        attention_dim=checkpoint['attention_dim'],\n",
        "        encoder_dim=checkpoint['encoder_dim'],\n",
        "        decoder_dim=checkpoint['decoder_dim'],\n",
        "        vocab=checkpoint['vocab']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWBOfeoSfcfQ",
        "outputId": "00af7731-d30f-4584-b329-891ec12e90bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load model successfully\n"
          ]
        }
      ],
      "source": [
        "model = load_model(\"models/luong_attn/model_best.pth\")\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "print(\"Load model successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7Yv-MYHGf5",
        "outputId": "2012a14e-6ce1-4c3b-a0d2-cef6802b1f39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "253it [07:45,  1.84s/it]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    list_of_references = []\n",
        "    hypotheses = []\n",
        "    bleu_score = []\n",
        "    for idx, (image, target) in tqdm(enumerate(iter(loader))):\n",
        "        image, target = image.to(device), target[:, 1:, :].tolist()\n",
        "\n",
        "\n",
        "        mapped_target = map_target(target)\n",
        "        list_of_references.extend(mapped_target)\n",
        "\n",
        "        predicted_captions = model.generate_caption_batch(image).tolist()\n",
        "        predicted_captions= list(map(map_predict, predicted_captions))\n",
        "\n",
        "        hypotheses.extend(predicted_captions)\n",
        "        score = corpus_bleu(list_of_references, hypotheses)\n",
        "        bleu_score.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmhG979AnqS8",
        "outputId": "53ac1c46-3f7f-44b4-ddb0-3054619b3abc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5891869092912313"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(bleu_score) / len(bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBG8z52L0bv"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ImVS7hoL8Xs"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr2UaG2IMOOW"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPjEvEi0MP2f"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout = 0.1, max_len = 50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        self.pe = torch.zeros(1, max_len, d_model).to(device)\n",
        "        self.pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbiiQ1OoL9We"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2ri12bqMKwz"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, encoder_dim, d_model):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Load pretrained model and remove last fc layer\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-2]).to(device)\n",
        "\n",
        "        # Freeze layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.linear = torch.nn.Linear(encoder_dim, d_model).to(device)\n",
        "    def forward(self, images):\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = self.model(images)\n",
        "        features = features.view(features.size(0), features.size(1), -1)\n",
        "        features = features.permute(0, 2, 1)\n",
        "        features = self.linear(features)\n",
        "        return features # (batch_size, 49, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLcIU0A8MLMI"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ_DkW0bML5A"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_tokens, d_model,\n",
        "                 n_heads, dim_forward,\n",
        "                 n_layers, dropout = 0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(n_tokens, d_model).to(device) # embedding layer\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout).to(device) # positional encoder\n",
        "\n",
        "        decoder_layers = TransformerDecoderLayer(d_model, n_heads, dim_forward, dropout, batch_first=True) # encoder layer\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layers, n_layers).to(device) # transformer encoder\n",
        "\n",
        "\n",
        "        self.d_model = d_model # number of features\n",
        "        self.linear = torch.nn.Linear(d_model, n_tokens).to(device) # last linear model for prediction\n",
        "\n",
        "    def forward(self, features, captions, padding_mask, captions_mask = None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            captions: Tensor, shape ``[batch_size, seq_len]``\n",
        "            captions_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[batch_size, seq_len, n_tokens]``\n",
        "        \"\"\"\n",
        "        captions = captions.to(device)\n",
        "        captions = self.embedding(captions)\n",
        "        captions = captions * math.sqrt(self.d_model)\n",
        "        captions = self.pos_encoder(captions)\n",
        "\n",
        "        if captions_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            captions_mask = torch.nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
        "\n",
        "        output = self.transformer_decoder(tgt=captions,\n",
        "                                          memory=features,\n",
        "                                          tgt_key_padding_mask=padding_mask,\n",
        "                                          tgt_mask=captions_mask)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def predict(self, feature, max_length, vocab):\n",
        "        word = torch.tensor([vocab.word2index['<SOS>']] + [0] * (max_length - 1)).view(1, -1).to(device)\n",
        "        padding_mask = torch.Tensor([True] * max_length).view(1, -1).to(device)\n",
        "\n",
        "        predicted_captions = []\n",
        "\n",
        "        for i in range(max_length - 1):\n",
        "            # Update the padding masks\n",
        "            padding_mask[:, i] = False\n",
        "\n",
        "            # Get the model prediction for the next word\n",
        "            output = self.forward(feature, word, padding_mask)\n",
        "            output = output[0, i]\n",
        "            predicted_word_idx = output.argmax(dim=-1)\n",
        "            predicted_captions.append(predicted_word_idx.item())\n",
        "            word[:, i + 1] = predicted_word_idx.item()\n",
        "\n",
        "            # End if <EOS> appears\n",
        "            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "        return ' '.join([vocab.index2word[idx] for idx in predicted_captions])\n",
        "\n",
        "    def predict_batch(self, features, max_length, vocab):\n",
        "        n_samples = features.size(0)\n",
        "\n",
        "        word = torch.tensor([vocab.word2index['<SOS>']] + [0] * (max_length - 1)).view(1, -1).to(device)\n",
        "        word = word.repeat(n_samples, 1)\n",
        "\n",
        "        padding_mask = torch.Tensor([True] * max_length).view(1, -1).to(device)\n",
        "        padding_mask = padding_mask.repeat(n_samples, 1)\n",
        "\n",
        "        predicted_captions = [[] for _ in range(n_samples)]\n",
        "        is_predicted = [False] * n_samples\n",
        "\n",
        "        for i in range(max_length - 1):\n",
        "            # Update the padding masks\n",
        "            padding_mask[:, i] = False\n",
        "\n",
        "            # Get the model prediction for the next word\n",
        "            output = self.forward(features, word, padding_mask)\n",
        "            output = output[torch.arange(n_samples), [i] * n_samples].clone()\n",
        "            predicted_word_idx = output.argmax(dim=-1)\n",
        "\n",
        "            for idx in range(n_samples):\n",
        "                if is_predicted[idx]:\n",
        "                    continue\n",
        "                predicted_captions[idx].append(predicted_word_idx[idx].item())\n",
        "                if predicted_word_idx[idx].item() == 2:\n",
        "                    is_predicted[idx] = True\n",
        "            if np.all(is_predicted):\n",
        "                break\n",
        "\n",
        "            word[torch.arange(n_samples), [i + 1] * n_samples] = predicted_word_idx.view(-1)\n",
        "        return predicted_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIfSBVvMTop"
      },
      "source": [
        "### Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyc-ULa4MURF"
      },
      "outputs": [],
      "source": [
        "class Captioner(torch.nn.Module):\n",
        "    def __init__(self, n_tokens, d_model, n_heads, dim_forward, n_layers, encoder_dim, vocab):\n",
        "        super(Captioner, self).__init__()\n",
        "        self.encoder =  Encoder(encoder_dim=encoder_dim,\n",
        "                                d_model=d_model)\n",
        "        self.decoder = Decoder(n_tokens=n_tokens,\n",
        "                               d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               dim_forward=dim_forward,\n",
        "                               n_layers=n_layers)\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, images, captions, padding_mask):\n",
        "\n",
        "        features = self.encoder(images)\n",
        "        output = self.decoder(features, captions, padding_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_caption(self, image, max_length=50):\n",
        "        image = image.to(device)\n",
        "        feature = self.encoder(image)\n",
        "        predicted_caption = self.decoder.predict(feature, max_length, self.vocab)\n",
        "        return predicted_caption\n",
        "\n",
        "\n",
        "    def generate_caption_batch(self, images, max_length=50):\n",
        "        images = images.to(device)\n",
        "        feature = self.encoder(images)\n",
        "        predicted_captions = self.decoder.predict_batch(feature, max_length, self.vocab)\n",
        "        return predicted_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwrCGmDMXFP"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-uB3udRMYBb"
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model = Captioner(\n",
        "        n_tokens=checkpoint['n_tokens'],\n",
        "        d_model=checkpoint['d_model'],\n",
        "        n_heads=checkpoint['n_heads'],\n",
        "        dim_forward=checkpoint['dim_forward'],\n",
        "        n_layers=checkpoint['n_layers'],\n",
        "        encoder_dim=checkpoint['encoder_dim'],\n",
        "        vocab=checkpoint['vocab'],\n",
        "\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL9QGK1CMeB3",
        "outputId": "bda4f753-c71a-4228-cd90-71ab11fe0ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load model successfully\n"
          ]
        }
      ],
      "source": [
        "model = load_model(\"models/transformer2/model_best.pth\")\n",
        "model.eval()\n",
        "print(\"Load model successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A54x4J-hMl10",
        "outputId": "9a94b7aa-a5da-4652-8a7e-ceebf2eac44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:02, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['a', 'woman', 'in', 'a', 'pink', 'dress', 'is', 'sitting', 'on', 'a', 'wooden', 'bench', '.'], ['two', 'dogs', 'are', 'playing', 'with', 'a', 'ball', 'on', 'the', 'grass', '.'], ['a', 'little', 'girl', 'is', 'sitting', 'on', 'a', 'yellow', 'and', 'yellow', 'tent', '.'], ['a', 'man', 'laying', 'on', 'a', 'bench', 'with', 'a', 'dog', 'on', 'his', 'back', '.'], ['a', 'man', 'with', 'a', 'mohawk', 'and', 'a', 'hat', 'is', 'wearing', 'a', 'hat', '.'], ['a', 'little', 'girl', 'in', 'a', 'red', 'shirt', 'is', 'swinging', 'a', 'rope', '.'], ['a', 'dog', 'running', 'in', 'the', 'grass', 'with', 'a', 'ball', 'in', 'its', 'mouth', '.'], ['a', 'white', 'dog', 'with', 'a', 'red', 'collar', 'is', 'running', 'on', 'the', 'beach', '.'], ['a', 'little', 'boy', 'is', 'standing', 'on', 'a', 'sidewalk', 'with', 'his', 'arms', 'outstretched', '.'], ['a', 'black', 'and', 'white', 'dog', 'is', 'jumping', 'over', 'a', 'log', '.'], ['a', 'white', 'dog', 'is', 'running', 'through', 'the', 'snow', '.'], ['a', 'man', 'in', 'a', 'hat', 'and', 'hat', 'is', 'standing', 'in', 'the', 'snow', '.'], ['a', 'man', 'in', 'red', 'is', 'climbing', 'a', 'rock', 'wall', '.'], ['a', 'brown', 'dog', 'is', 'running', 'on', 'a', 'grassy', 'field', '.'], ['a', 'white', 'dog', 'is', 'running', 'on', 'the', 'grass', '.'], ['a', 'white', 'dog', 'jumps', 'to', 'catch', 'a', 'tennis', 'ball', '.'], ['a', 'man', 'is', 'standing', 'on', 'a', 'dock', 'and', 'a', 'woman', 'in', 'the', 'water', '.'], ['a', 'group', 'of', 'people', 'sit', 'on', 'a', 'bench', 'near', 'a', 'tree', '.'], ['a', 'black', 'dog', 'is', 'running', 'through', 'the', 'water', '.'], ['a', 'man', 'in', 'a', 'white', 'shirt', 'and', 'black', 'pants', 'is', 'standing', 'on', 'a', 'beach', '.'], ['a', 'brown', 'dog', 'is', 'running', 'through', 'the', 'snow', '.'], ['a', 'person', 'in', 'a', 'red', 'jacket', 'is', 'climbing', 'up', 'a', 'snowy', 'hill', '.'], ['a', 'black', 'dog', 'is', 'carrying', 'a', 'black', 'dog', 'in', 'its', 'mouth', '.'], ['a', 'man', 'and', 'a', 'woman', 'are', 'paddling', 'a', 'yellow', 'kayak', 'in', 'a', 'yellow', 'kayak', '.'], ['a', 'brown', 'dog', 'is', 'playing', 'with', 'a', 'red', 'toy', 'in', 'the', 'grass', '.'], ['a', 'child', 'in', 'a', 'red', 'coat', 'is', 'playing', 'on', 'a', 'playground', '.'], ['a', 'man', 'in', 'a', 'black', 'shirt', 'is', 'standing', 'in', 'front', 'of', 'a', 'large', 'building', '.'], ['two', 'people', 'are', 'walking', 'along', 'the', 'beach', '.'], ['three', 'dogs', 'are', 'running', 'on', 'the', 'grass', '.'], ['a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'large', 'building', '.'], ['a', 'boy', 'eats', 'a', 'food', 'at', 'a', 'table', '.'], ['three', 'people', 'sit', 'on', 'a', 'mountain', 'overlooking', 'a', 'valley', '.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    list_of_references = []\n",
        "    hypotheses = []\n",
        "    bleu_score = []\n",
        "    for idx, (images, targets) in tqdm(enumerate(iter(loader))):\n",
        "        images, targets = images.to(device), targets[:, 1:, :].tolist()\n",
        "\n",
        "\n",
        "        mapped_target = map_target(targets)\n",
        "        list_of_references.extend(mapped_target)\n",
        "\n",
        "        predicted_captions = model.generate_caption_batch(images)\n",
        "        predicted_captions= list(map(map_predict, predicted_captions))\n",
        "\n",
        "        hypotheses.extend(predicted_captions)\n",
        "        score = corpus_bleu(list_of_references, hypotheses)\n",
        "        bleu_score.append(score)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVLdficgXOJ9",
        "outputId": "2f212200-3a46-4004-b60d-ca13a80fdbd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29821697109124284"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sum(bleu_score) / len(bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8JK72caFXlo"
      },
      "source": [
        "# Par-Inject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfYZDGPmFZKq"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk0Qy6F2FbOL"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VUGjIMpFaEb"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, encoder_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "\n",
        "        # Load pretrained model and remove last fc layer\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-1]).to(device)\n",
        "\n",
        "        # Freeze layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add a linear layer add the end of model\n",
        "        self.linear = torch.nn.Linear(2048, self.encoder_dim).to(device)\n",
        "        self.drop = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Preprocess images\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        features = self.model(images)                     # (batch_size, 2048, 1, 1)\n",
        "        features = features.view(images.shape[0], 1, -1)  # (batch_size, 1, 2048)\n",
        "        features = self.linear(self.drop(features))       # (batch_size, 1, 512)\n",
        "        features = features.squeeze(1)                    # (batch_size, 512)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Ee2hW6FeKc"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew71jJSwFeuz"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, encoder_dim, decoder_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = torch.nn.LSTMCell(input_size=embed_dim + encoder_dim, hidden_size=decoder_dim).to(device)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear1 = torch.nn.Linear(decoder_dim, decoder_dim).to(device)\n",
        "        self.linear2 = torch.nn.Linear(decoder_dim, vocab_size).to(device)\n",
        "        self.drop = torch.nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "    def init_hidden_state(self, features):\n",
        "        hidden = torch.zeros(features.size(0), self.decoder_dim).to(device)\n",
        "        cell = torch.zeros(features.size(0), self.decoder_dim).to(device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "    def forward_step(self, embed_words, features, hidden, cell):\n",
        "        lstm_input = torch.cat((embed_words, features), dim=1)\n",
        "        hidden, cell = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        decoded = self.linear1(hidden)\n",
        "        decoded = self.drop(decoded)\n",
        "        output = self.linear2(decoded)\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def forward(self, features, sequences):\n",
        "\n",
        "        sequence_length = len(sequences[0]) - 1\n",
        "        preds = torch.zeros(sequences.shape[0], sequence_length, self.vocab_size)\n",
        "\n",
        "        sequences = sequences.to(device)\n",
        "        preds = preds.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(sequences)\n",
        "        embeds = embeds.to(torch.float32)\n",
        "\n",
        "        hidden, cell = self.init_hidden_state(features)\n",
        "\n",
        "        # Forward pass\n",
        "        for idx in range(sequence_length):\n",
        "            # Compute feature vector of input text\n",
        "            embed_words = embeds[:, idx]\n",
        "\n",
        "            output, hidden, cell = self.forward_step(embed_words, features, hidden, cell)\n",
        "\n",
        "            # Predicted vector\n",
        "            preds[:, idx] = output\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def predict(self, feature, max_length=20, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.tensor(vocab.word2index['<SOS>']).view(1, -1)\n",
        "        word = word.to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        captions = []\n",
        "\n",
        "        hidden, cell = self.init_hidden_state(feature)\n",
        "\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden, cell = self.forward_step(embed_word, feature, hidden, cell)\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            # End if <EOS> appears\n",
        "            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        # Convert the vocab idx to words and return sentence\n",
        "        return ' '.join([vocab.index2word[idx] for idx in captions])\n",
        "\n",
        "\n",
        "    def predict_batch(self, features, max_length=20, vocab=None):\n",
        "        word = torch.full((features.shape[0], 1), vocab.word2index['<SOS>']).to(device)\n",
        "        features = features.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "        predicted_captions = torch.zeros(max_length, features.shape[0])\n",
        "        hidden, cell = self.init_hidden_state(features)\n",
        "\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_words = embeds[:, 0]\n",
        "            output, hidden, cell = self.forward_step(embed_words, features, hidden, cell)\n",
        "\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            predicted_captions[idx, :] = predicted_word_idx.unsqueeze(0)[:, :]\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(1))\n",
        "        predicted_captions = predicted_captions.permute(1, 0)\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrCmbQzFhgB"
      },
      "source": [
        "### Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuRPgeQgFisl"
      },
      "outputs": [],
      "source": [
        "class Captioner(torch.nn.Module):\n",
        "    def __init__(self, vocab_size,  vocab, embed_dim, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        self.encoder =  Encoder(encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, embed_dim, encoder_dim, decoder_dim)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        image_fv = self.encoder(images)\n",
        "        output = self.decoder(image_fv, captions)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_caption(self, image, max_length=20):\n",
        "        feature = self.encoder(image)\n",
        "        predicted_caption = self.decoder.predict(feature, max_length, self.vocab)\n",
        "\n",
        "        return predicted_caption\n",
        "\n",
        "    def generate_caption_batch(self, images, max_length=20):\n",
        "        features = self.encoder(images)\n",
        "        predicted_captions = self.decoder.predict_batch(features, max_length, self.vocab)\n",
        "\n",
        "        return predicted_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gbf1nM5FlEY"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-F1muELFmW2"
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model = Captioner(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        vocab=checkpoint['vocab'],\n",
        "        embed_dim=checkpoint['embed_dim'],\n",
        "        encoder_dim=checkpoint['encoder_dim'],\n",
        "        decoder_dim=checkpoint['decoder_dim'],\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAX3jLJdI-cQ",
        "outputId": "050e542e-a95c-472e-c572-45629ad6de0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load model successfully\n"
          ]
        }
      ],
      "source": [
        "model = load_model(\"models/par_inject/model_best.pth\")\n",
        "model.eval()\n",
        "print(\"Load model successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBarQfUpJBrF",
        "outputId": "7d3c3e82-cf01-420f-9e21-c84adb68c20d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "253it [08:20,  1.98s/it]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    list_of_references = []\n",
        "    hypotheses = []\n",
        "    bleu_score = []\n",
        "    for idx, (image, target) in tqdm(enumerate(iter(loader))):\n",
        "        image, target = image.to(device), target[:, 1:, :].tolist()\n",
        "\n",
        "\n",
        "        mapped_target = map_target(target)\n",
        "        list_of_references.extend(mapped_target)\n",
        "\n",
        "        predicted_captions = model.generate_caption_batch(image).tolist()\n",
        "        predicted_captions = list(map(map_predict, predicted_captions))\n",
        "\n",
        "        hypotheses.extend(predicted_captions)\n",
        "        score = corpus_bleu(list_of_references, hypotheses)\n",
        "        bleu_score.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe8pCsBKJC89",
        "outputId": "e56a3832-0405-4688-bb23-3be2f27e3741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.34457920062888253"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(bleu_score) / len(bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjX8ywu1TmQr"
      },
      "source": [
        "# Init-Inject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4NCsyedTpX8"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzwfy9TDXXPo"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, encoder_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "\n",
        "        # Load pretrained model and remove last fc layer\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-1]).to(device)\n",
        "\n",
        "        # Freeze layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add a linear layer add the end of model\n",
        "        self.linear = torch.nn.Linear(2048, self.encoder_dim).to(device)\n",
        "        self.drop = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Preprocess images\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        features = self.model(images)                     # (batch_size, 2048, 1, 1)\n",
        "        features = features.view(images.shape[0], 1, -1)  # (batch_size, 1, 2048)\n",
        "        features = self.linear(self.drop(features))       # (batch_size, 1, 512)\n",
        "        features = features.squeeze(1)                    # (batch_size, 512)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2hOnpNZXVB3"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, encoder_dim, decoder_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim).to(device)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = torch.nn.LSTMCell(input_size=embed_dim, hidden_size=decoder_dim).to(device)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear1 = torch.nn.Linear(decoder_dim, decoder_dim).to(device)\n",
        "        self.linear2 = torch.nn.Linear(decoder_dim, vocab_size).to(device)\n",
        "        self.drop = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def init_hidden_state(self, features):\n",
        "        hidden = features\n",
        "        cell = features\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward_step(self, embed_words, features, hidden, cell):\n",
        "        hidden, cell = self.lstm(embed_words, (hidden, cell))\n",
        "\n",
        "        decoded = self.linear1(hidden)\n",
        "        decoded = self.drop(decoded)\n",
        "        output = self.linear2(decoded)\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def forward(self, features, sequences):\n",
        "\n",
        "        sequence_length = len(sequences[0]) - 1\n",
        "        preds = torch.zeros(sequences.shape[0], sequence_length, self.vocab_size)\n",
        "\n",
        "        sequences = sequences.to(device)\n",
        "        preds = preds.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(sequences)\n",
        "        embeds = embeds.to(torch.float32)\n",
        "\n",
        "        hidden, cell = self.init_hidden_state(features)\n",
        "\n",
        "        # Forward pass\n",
        "        for idx in range(sequence_length):\n",
        "            # Compute feature vector of input text\n",
        "            embed_words = embeds[:, idx]\n",
        "\n",
        "            output, hidden, cell = self.forward_step(embed_words, features, hidden, cell)\n",
        "\n",
        "            # Predicted vector\n",
        "            preds[:, idx] = output\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def predict(self, feature, max_length=20, vocab=None):\n",
        "        # Starting input\n",
        "        word = torch.tensor(vocab.word2index['<SOS>']).view(1, -1).to(device)\n",
        "        feature = feature.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        captions = []\n",
        "\n",
        "        hidden, cell = self.init_hidden_state(feature)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_word = embeds[:, 0]\n",
        "            output, hidden, cell = self.forward_step(embed_word, feature, hidden, cell)\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            # End if <EOS> appears\n",
        "            if vocab.index2word[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        # Convert the vocab idx to words and return sentence\n",
        "        return ' '.join([vocab.index2word[idx] for idx in captions])\n",
        "\n",
        "    def predict_batch(self, features, max_length=20, vocab=None):\n",
        "        word = torch.full((features.shape[0], 1), vocab.word2index['<SOS>']).to(device)\n",
        "        features = features.to(device)\n",
        "\n",
        "        # Embedding sequence\n",
        "        embeds = self.embedding(word)\n",
        "        predicted_captions = torch.zeros(max_length, features.shape[0])\n",
        "        hidden, cell = self.init_hidden_state(features)\n",
        "\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            embed_words = embeds[:, 0]\n",
        "            output, hidden, cell = self.forward_step(embed_words, features, hidden, cell)\n",
        "\n",
        "            # Predict word index\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            predicted_captions[idx, :] = predicted_word_idx.unsqueeze(0)[:, :]\n",
        "\n",
        "            # Send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(1))\n",
        "        predicted_captions = predicted_captions.permute(1, 0)\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sal06z8RT5g2"
      },
      "outputs": [],
      "source": [
        "class Captioner(torch.nn.Module):\n",
        "    def __init__(self, vocab_size,  vocab, embed_dim, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        self.encoder =  Encoder(encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, embed_dim, encoder_dim, decoder_dim)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        image_fv = self.encoder(images)\n",
        "        output = self.decoder(image_fv, captions)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_caption(self, image, max_length=20):\n",
        "        feature = self.encoder(image)\n",
        "        predicted_caption = self.decoder.predict(feature, max_length, self.vocab)\n",
        "\n",
        "        return predicted_caption\n",
        "\n",
        "    def generate_caption_batch(self, images, max_length=20):\n",
        "        features = self.encoder(images)\n",
        "        predicted_captions = self.decoder.predict_batch(features, max_length, self.vocab)\n",
        "\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1xhQOGxTrIp"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqSXzcXUTqPC"
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model = Captioner(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        vocab=checkpoint['vocab'],\n",
        "        embed_dim=checkpoint['embed_dim'],\n",
        "        encoder_dim=checkpoint['encoder_dim'],\n",
        "        decoder_dim=checkpoint['decoder_dim'],\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BG5nUKPTqeq",
        "outputId": "b5ff68ee-fbea-446f-e966-a313dc2eead7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load model successfully\n"
          ]
        }
      ],
      "source": [
        "model = load_model(\"models/init_inject/model_best.pth\")\n",
        "model.eval()\n",
        "print(\"Load model successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol185iYwT1qV",
        "outputId": "04a71cc3-0106-4a94-949f-bfb075389174"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "253it [09:32,  2.26s/it]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    list_of_references = []\n",
        "    hypotheses = []\n",
        "    bleu_score = []\n",
        "    for idx, (image, target) in tqdm(enumerate(iter(loader))):\n",
        "        image, target = image.to(device), target[:, 1:, :].tolist()\n",
        "\n",
        "\n",
        "        mapped_target = map_target(target)\n",
        "        list_of_references.extend(mapped_target)\n",
        "\n",
        "        predicted_captions = model.generate_caption_batch(image).tolist()\n",
        "        predicted_captions = list(map(map_predict, predicted_captions))\n",
        "\n",
        "        hypotheses.extend(predicted_captions)\n",
        "        score = corpus_bleu(list_of_references, hypotheses)\n",
        "        bleu_score.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoyBnfmzT4nU",
        "outputId": "1f95f73a-1bf5-4512-fd62-e275120f40f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.27340657084194314"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(bleu_score) / len(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM1vgsmsYiAu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nNDmiaTFOamQ",
        "GiBP0kLcqLe9",
        "ZkXk51z8OcZi",
        "APrvh4KbLunt",
        "gNig4R6wR-Ko",
        "2_y6uEWHdCjT",
        "WbI3aw5_XwNj",
        "wpsk7dsFSqbc",
        "zO_HFiZzdGNH",
        "nm2wZt8MG_0Q",
        "-ImVS7hoL8Xs",
        "yr2UaG2IMOOW",
        "KbiiQ1OoL9We",
        "TLcIU0A8MLMI",
        "4qIfSBVvMTop",
        "W8JK72caFXlo",
        "NfYZDGPmFZKq",
        "vk0Qy6F2FbOL",
        "0WrCmbQzFhgB",
        "2gbf1nM5FlEY",
        "rjX8ywu1TmQr"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}